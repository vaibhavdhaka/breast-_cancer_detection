{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F0YH3AlnUDF0"
   },
   "source": [
    "# Breast Cancer Detection\n",
    "\n",
    "- categories: [machine_learning, scikit-learn, logistic_regression, kNN, SVM, decision_tree, random_forest, adaboost, naive_bayes, quadratic_discriminant_analysis, neural_network, gaussian_process, breast_cancer_detection, structured_data, uci_dataset]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N3j4nP0qUDF3"
   },
   "source": [
    "We will look at application of Machine Learning algorithms to one of the data sets from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php) to classify whether a set of readings from clinical reports are positive for breast cancer or not.\n",
    "\n",
    "This is one of the easier datasets to process since all the features have integer values.\n",
    "\n",
    "We will use the [scikit-learn](https://scikit-learn.org/stable/) algorithms to process this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q_B3sUsHUDF3"
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kwe5vhkqUDF3"
   },
   "source": [
    "We will use the [Breast Cancer Wisconsin (Original) Data Set](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Original%29)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UEmOOP4pUDF4"
   },
   "source": [
    "### Details about the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_yxxIfp_UDF4"
   },
   "source": [
    "- Data Set Characteristics: Multivariate\n",
    "- Attribute Characteristics: Integer\n",
    "- Associated Tasks: Classification\n",
    "- Number of instances: 699\n",
    "- Number of attributes: 10\n",
    "- Area: Life"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M7V80bMXUDF4"
   },
   "source": [
    "### Attribute Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vy0XXtgzUDF4"
   },
   "source": [
    "1. Sample code number: id number\n",
    "2. Clump Thickness: 1 - 10\n",
    "3. Uniformity of Cell Size: 1 - 10\n",
    "4. Uniformity of Cell Shape: 1 - 10\n",
    "5. Marginal Adhesion: 1 - 10\n",
    "6. Single Epithelial Cell Size: 1 - 10\n",
    "7. Bare Nuclei: 1 - 10\n",
    "8. Bland Chromatin: 1 - 10\n",
    "9. Normal Nucleoli: 1 - 10\n",
    "10. Mitoses: 1 - 10\n",
    "11. Class: (2 for benign, 4 for malignant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DOMkMaLeUDF5"
   },
   "source": [
    "### Class Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4YNM-T1BUDF5"
   },
   "source": [
    "- Benign: 458 (65.5%)\n",
    "- Malignant: 241 (34.5%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tggXHGOlUDF5"
   },
   "source": [
    "### Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RuhL6bHBUDF5"
   },
   "source": [
    "There are 16 instances in Groups 1 to 6 that contain a single missing (i.e., unavailable) attribute value, denoted by \"?\".  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Ay-h10YUDF5"
   },
   "source": [
    "## Prepare Dataset for Machine Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6QKGsI-2UDF5"
   },
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "5fS7wwVLUDF5",
    "outputId": "14dc9375-0944-45d8-8573-0269e1ab4cda",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read in 569 rows\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0  1      2      3       4       5        6        7       8   \\\n",
       "0    842302  M  17.99  10.38  122.80  1001.0  0.11840  0.27760  0.3001   \n",
       "1    842517  M  20.57  17.77  132.90  1326.0  0.08474  0.07864  0.0869   \n",
       "2  84300903  M  19.69  21.25  130.00  1203.0  0.10960  0.15990  0.1974   \n",
       "3  84348301  M  11.42  20.38   77.58   386.1  0.14250  0.28390  0.2414   \n",
       "4  84358402  M  20.29  14.34  135.10  1297.0  0.10030  0.13280  0.1980   \n",
       "\n",
       "        9   ...     22     23      24      25      26      27      28      29  \\\n",
       "0  0.14710  ...  25.38  17.33  184.60  2019.0  0.1622  0.6656  0.7119  0.2654   \n",
       "1  0.07017  ...  24.99  23.41  158.80  1956.0  0.1238  0.1866  0.2416  0.1860   \n",
       "2  0.12790  ...  23.57  25.53  152.50  1709.0  0.1444  0.4245  0.4504  0.2430   \n",
       "3  0.10520  ...  14.91  26.50   98.87   567.7  0.2098  0.8663  0.6869  0.2575   \n",
       "4  0.10430  ...  22.54  16.67  152.20  1575.0  0.1374  0.2050  0.4000  0.1625   \n",
       "\n",
       "       30       31  \n",
       "0  0.4601  0.11890  \n",
       "1  0.2750  0.08902  \n",
       "2  0.3613  0.08758  \n",
       "3  0.6638  0.17300  \n",
       "4  0.2364  0.07678  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data', header=None) #file contains no header info\n",
    "print(f\"Read in {len(df)} rows\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YAN5Bj4hUDF6"
   },
   "source": [
    "### Deal with Missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tc8lUlVCUDF6"
   },
   "source": [
    "There are 16 missing attribute values and we need to deal with them as they have the value '?' in them. We need integer values for processing the data. So let's deal with the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "mKtgf9erUDF6"
   },
   "outputs": [],
   "source": [
    "df.replace(\"?\", 10000, inplace=True) #10,000 is way beyond the range of columns provided so acts as an outlier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kML86SwHUDF6"
   },
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J1ZVabiSUDF6"
   },
   "source": [
    "The first column in the dataset is defined to be a \"sample code number\". This column should not have any bearing on the outcome of the test. So we will drop this column from our dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>843786</td>\n",
       "      <td>M</td>\n",
       "      <td>12.45</td>\n",
       "      <td>15.70</td>\n",
       "      <td>82.57</td>\n",
       "      <td>477.1</td>\n",
       "      <td>0.12780</td>\n",
       "      <td>0.17000</td>\n",
       "      <td>0.1578</td>\n",
       "      <td>0.08089</td>\n",
       "      <td>...</td>\n",
       "      <td>15.47</td>\n",
       "      <td>23.75</td>\n",
       "      <td>103.40</td>\n",
       "      <td>741.6</td>\n",
       "      <td>0.1791</td>\n",
       "      <td>0.5249</td>\n",
       "      <td>0.5355</td>\n",
       "      <td>0.1741</td>\n",
       "      <td>0.3985</td>\n",
       "      <td>0.12440</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0  1      2      3       4       5        6        7       8   \\\n",
       "1    842517  M  20.57  17.77  132.90  1326.0  0.08474  0.07864  0.0869   \n",
       "2  84300903  M  19.69  21.25  130.00  1203.0  0.10960  0.15990  0.1974   \n",
       "3  84348301  M  11.42  20.38   77.58   386.1  0.14250  0.28390  0.2414   \n",
       "4  84358402  M  20.29  14.34  135.10  1297.0  0.10030  0.13280  0.1980   \n",
       "5    843786  M  12.45  15.70   82.57   477.1  0.12780  0.17000  0.1578   \n",
       "\n",
       "        9   ...     22     23      24      25      26      27      28      29  \\\n",
       "1  0.07017  ...  24.99  23.41  158.80  1956.0  0.1238  0.1866  0.2416  0.1860   \n",
       "2  0.12790  ...  23.57  25.53  152.50  1709.0  0.1444  0.4245  0.4504  0.2430   \n",
       "3  0.10520  ...  14.91  26.50   98.87   567.7  0.2098  0.8663  0.6869  0.2575   \n",
       "4  0.10430  ...  22.54  16.67  152.20  1575.0  0.1374  0.2050  0.4000  0.1625   \n",
       "5  0.08089  ...  15.47  23.75  103.40   741.6  0.1791  0.5249  0.5355  0.1741   \n",
       "\n",
       "       30       31  \n",
       "1  0.2750  0.08902  \n",
       "2  0.3613  0.08758  \n",
       "3  0.6638  0.17300  \n",
       "4  0.2364  0.07678  \n",
       "5  0.3985  0.12440  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if the label exists in the specified axis before attempting to drop\n",
    "if 0 in df.index:\n",
    "    df.drop(labels=[0], axis=0, inplace=True)  # Dropping the first row\n",
    "elif 0 in df.columns:\n",
    "    df.drop(labels=[0], axis=1, inplace=True)  # Dropping the first column\n",
    "\n",
    "# Display the DataFrame to verify the result\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZBmEr6REUDF7"
   },
   "source": [
    "### Split dataset into Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (426, 31)\n",
      "X_test shape: (142, 31)\n",
      "y_train shape: (426,)\n",
      "y_test shape: (142,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Verify if the column exists in the DataFrame\n",
    "if 10 in df.columns:\n",
    "    # Drop the column with label 10 to create the feature matrix X\n",
    "    X = np.array(df.drop(labels=[10], axis=1)) # Use keyword arguments\n",
    "    # The target vector y is the column with label 10\n",
    "    y = np.array(df[10]) # Access the column directly\n",
    "\n",
    "    # Split the dataset into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=43)\n",
    "else:\n",
    "    print(\"The column with label 10 does not exist in the DataFrame.\")\n",
    "\n",
    "# Display the shapes of the train and test sets\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r2mN1Z5dUDF7"
   },
   "source": [
    "## Train Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NciWLPhRUDF7"
   },
   "source": [
    "We will train different models from the sklearn library on this data and see how each one performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "3p09yOACUDF7"
   },
   "outputs": [],
   "source": [
    "names = [\"Logistic Regression\", \"Nearest Neighbors\", \"Linear SVM\", \"RBF SVM\", \"Gaussian Process\",\n",
    "         \"Decision Tree\", \"Random Forest\", \"Neural Net\", \"AdaBoost\",\n",
    "         \"Naive Bayes\", \"QDA\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "eQnPvSvkUDF7"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "-2zgQUjmUDF7"
   },
   "outputs": [],
   "source": [
    "classifiers = [\n",
    "    LogisticRegression(max_iter=300),\n",
    "    KNeighborsClassifier(),\n",
    "    SVC(kernel=\"linear\", C=0.025),\n",
    "    SVC(gamma=2, C=1),\n",
    "    GaussianProcessClassifier(1.0 * RBF(1.0)),\n",
    "    DecisionTreeClassifier(max_depth=5, random_state=43),\n",
    "    RandomForestClassifier(max_depth=5, random_state=43),\n",
    "    MLPClassifier(alpha=1, max_iter=1000),\n",
    "    AdaBoostClassifier(),\n",
    "    GaussianNB(),\n",
    "    QuadraticDiscriminantAnalysis()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Logistic Regression Classifier is: 0.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Sample data\n",
    "data = {\n",
    "    'feature1': [1, 2, 3, 4],\n",
    "    'feature2': ['A', 'B', 'A', 'C'],\n",
    "    'target': [0, 1, 0, 1]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Split the data into features and target\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "\n",
    "# Identify categorical features\n",
    "categorical_features = ['feature2']\n",
    "numerical_features = ['feature1']\n",
    "\n",
    "# Preprocessing pipeline for numerical features\n",
    "numerical_transformer = SimpleImputer(strategy='constant')\n",
    "\n",
    "# Preprocessing pipeline for categorical features\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Bundle preprocessing for numerical and categorical features\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Define the model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Create and evaluate the pipeline\n",
    "clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('classifier', model)])\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Fit the model\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Score the model\n",
    "score = clf.score(X_test, y_test)\n",
    "print(f\"Accuracy of Logistic Regression Classifier is: {score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "yGa3SvRuUDF7",
    "outputId": "96a86820-7cf8-4d0e-8ca0-d27ad64fdde1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The column with label 10 does not exist in the DataFrame.\n"
     ]
    }
   ],
   "source": [
    "# iterate over classifiers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Assuming df is your DataFrame and column 10 contains the target labels\n",
    "\n",
    "# Check if the column exists in the DataFrame\n",
    "if 10 in df.columns:\n",
    "    # Convert categorical data to numeric using get_dummies\n",
    "    df_encoded = pd.get_dummies(df)\n",
    "    \n",
    "    # Create feature matrix X and target vector y\n",
    "    X = df_encoded.drop(columns=[10]).values\n",
    "    y = df_encoded[10].values\n",
    "\n",
    "    # Split the dataset into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=43)\n",
    "\n",
    "    # Scale the data (optional, but recommended for many classifiers)\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    # Iterate over classifiers\n",
    "    for name, clf in zip(names, classifiers):\n",
    "        clf.fit(X_train, y_train)\n",
    "        score = clf.score(X_test, y_test)\n",
    "        print(f\"Accuracy of {name} Classifier is: {score}\")\n",
    "\n",
    "else:\n",
    "    print(\"The column with label 10 does not exist in the DataFrame.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n8nnnJo_UDF7"
   },
   "source": [
    "Now lets see if we can improve the performance of these algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cDMuCeBKUDF7"
   },
   "source": [
    "Standardize features by removing the mean and scaling to unit variance. Will be used with some of the algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "TT-Kh8YmUDF8"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Sample data (assuming df is your DataFrame and column 10 contains the target labels)\n",
    "# df = pd.read_csv('your_data.csv')\n",
    "\n",
    "# Check if the column exists in the DataFrame\n",
    "if 10 in df.columns:\n",
    "    # Convert categorical data to numeric using get_dummies\n",
    "    df_encoded = pd.get_dummies(df)\n",
    "    \n",
    "    # Create feature matrix X and target vector y\n",
    "    X = df_encoded.drop(columns=[10]).values\n",
    "    y = df_encoded[10].values\n",
    "\n",
    "    # Split the dataset into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=43)\n",
    "\n",
    "    # Scale the data\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Example classifiers list\n",
    "    names = [\"Logistic Regression\"]\n",
    "    classifiers = [LogisticRegression()]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CVgsSXrqUDF8"
   },
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Logistic Regression Classifier is: 1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Example DataFrame creation (replace this with your actual data loading step)\n",
    "# df = pd.read_csv('your_data.csv')\n",
    "\n",
    "# Ensure the target column is correctly specified\n",
    "target_column = 'target'  # replace 'target' with the actual name of your target column\n",
    "\n",
    "# Check if the target column exists in the DataFrame\n",
    "if target_column not in df.columns:\n",
    "    raise ValueError(f\"Target column '{target_column}' not found in DataFrame\")\n",
    "\n",
    "# Separate features and target variable\n",
    "X = df.drop(columns=[target_column])\n",
    "y = df[target_column]\n",
    "\n",
    "# Convert categorical features to numeric using one-hot encoding\n",
    "X_encoded = pd.get_dummies(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.25, random_state=43)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Initialize and fit the Logistic Regression model\n",
    "lr_model = LogisticRegression(random_state=43, max_iter=500, n_jobs=-1)  # Removed class_weight\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "lr_accuracy = lr_model.score(X_test_scaled, y_test)\n",
    "print(f\"Accuracy of Logistic Regression Classifier is: {lr_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dg0gCeANUDF8"
   },
   "source": [
    "We managed an improvement of over 1.7% in the overall accuracy score from 97.14% to 98.85%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ocOAgZNJUDF8"
   },
   "source": [
    "### K Nearest Neighbor Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "7ovbIDSLUDF8",
    "outputId": "7886aec5-99e9-4b34-b6ae-2c376afc5905"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of kNN Classifier is:0.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn_model = KNeighborsClassifier(n_neighbors=3, n_jobs=-1)\n",
    "knn_model.fit(X_train_scaled, y_train)\n",
    "knn_accuracy = knn_model.score(X_test_scaled, y_test)\n",
    "print(f\"Accuracy of kNN Classifier is:{knn_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4TmYhN7ZUDF8"
   },
   "source": [
    "We managed an improvement of over 1.1% in the overall accuracy score from 95.42% to 96.57%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i-E7NwoVUDF8"
   },
   "source": [
    "### Linear Support Vector Machines (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Linear SVM Classifier is: 0.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "# Correct class_weight parameter by removing it or setting it correctly\n",
    "# If you want to handle class imbalance, use 'balanced'\n",
    "# svm_model = svm.SVC(random_state=43, kernel='linear', class_weight='balanced')\n",
    "\n",
    "# If you don't need class weights, simply remove the parameter\n",
    "svm_model = svm.SVC(random_state=43, kernel='linear')\n",
    "\n",
    "# Fit the model\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "svm_accuracy = svm_model.score(X_test, y_test)\n",
    "print(f\"Accuracy of Linear SVM Classifier is: {svm_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F-e46YcwUDF_"
   },
   "source": [
    "We managed an improvement of over 2.8% in the overall accuracy score from 96.00% to 98.85%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "480v3daUUDF_"
   },
   "source": [
    "### RBF Support Vector Machines (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters are {'C': 0.01, 'gamma': 100.0} with a score of 1.00\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid\n",
    "C_range = np.logspace(-2, 10, 13)\n",
    "gamma_range = np.logspace(-9, 3, 13)\n",
    "param_grid = dict(gamma=gamma_range, C=C_range)\n",
    "\n",
    "# Use regular ShuffleSplit instead of StratifiedShuffleSplit\n",
    "cv = ShuffleSplit(n_splits=5, test_size=0.2, random_state=43)\n",
    "\n",
    "# Initialize GridSearchCV with SVC\n",
    "grid = GridSearchCV(svm.SVC(), param_grid=param_grid, cv=cv)\n",
    "grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Print the best parameters and the best score\n",
    "print(\"The best parameters are %s with a score of %0.2f\" % (grid.best_params_, grid.best_score_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique classes in y_train: [0 1]\n",
      "Accuracy of RBF SVM Classifier is: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from collections import Counter\n",
    "\n",
    "# Check unique classes in y_train\n",
    "unique_classes = np.unique(y_train)\n",
    "print(f\"Unique classes in y_train: {unique_classes}\")\n",
    "\n",
    "# Adjust the class_weight parameter based on the actual classes\n",
    "class_weights = {cls: 1 for cls in unique_classes}\n",
    "class_weights[4] = 2  # Adjust the weight for class 4 if it exists\n",
    "\n",
    "# Initialize and fit the SVM model with RBF kernel\n",
    "rbf_svm_model = svm.SVC(gamma=0.01, C=100, class_weight=class_weights)\n",
    "rbf_svm_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "rbf_svm_accuracy = rbf_svm_model.score(X_test_scaled, y_test)\n",
    "print(f\"Accuracy of RBF SVM Classifier is: {rbf_svm_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique classes in y_train: [0 1]\n",
      "Accuracy of RBF SVM Classifier is: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Check unique classes in y_train\n",
    "unique_classes = np.unique(y_train)\n",
    "print(f\"Unique classes in y_train: {unique_classes}\")\n",
    "\n",
    "# Specify class_weight only for existing classes\n",
    "class_weights = {4: 2} if 4 in unique_classes else {}\n",
    "\n",
    "# Initialize and fit the SVM model with RBF kernel\n",
    "rbf_svm_model = svm.SVC(gamma=0.01, C=100, class_weight=class_weights)\n",
    "rbf_svm_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "rbf_svm_accuracy = rbf_svm_model.score(X_test_scaled, y_test)\n",
    "print(f\"Accuracy of RBF SVM Classifier is: {rbf_svm_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ln2sHWQvUDGA"
   },
   "source": [
    "We managed an improvement of over 14.2% in the overall accuracy score from 84.00% to 98.28%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c0NJUEWWUDGA"
   },
   "source": [
    "### Gaussian Process Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "sxt6AWetUDGA",
    "outputId": "542acd23-21e2-4f65-99ec-192ea0831179"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Gaussian Process Classifier is:1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "\n",
    "kernel = 1.0 * RBF(1.0)\n",
    "gpc_model = GaussianProcessClassifier(kernel, random_state=43, max_iter_predict=1000, n_jobs=-1)\n",
    "gpc_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "gpc_accuracy = gpc_model.score(X_test_scaled, y_test)\n",
    "print(f\"Accuracy of Gaussian Process Classifier is:{gpc_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gtNQAgn1UDGA"
   },
   "source": [
    "We managed an improvement of over 0.5% in the overall accuracy score from 96.00% to 96.57%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SiqOMFz5UDGA"
   },
   "source": [
    "### Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique classes in y_train: [0 1]\n",
      "The best parameters are {'max_depth': 1} with a score of 0.67\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Check unique classes in y_train\n",
    "unique_classes = np.unique(y_train)\n",
    "print(f\"Unique classes in y_train: {unique_classes}\")\n",
    "\n",
    "# Only set class weights for classes that exist in y_train\n",
    "class_weights = {cls: 1 for cls in unique_classes}\n",
    "if 4 in unique_classes:\n",
    "    class_weights[4] = 2\n",
    "\n",
    "# Define the parameter grid for GridSearchCV\n",
    "max_depth_range = np.linspace(1, 10, 10, dtype=int)\n",
    "param_grid = dict(max_depth=max_depth_range)\n",
    "\n",
    "# Check the number of samples in X_train\n",
    "n_samples = X_train.shape[0]\n",
    "\n",
    "# Set the number of splits to be less than or equal to the number of samples\n",
    "n_splits = min(5, n_samples)\n",
    "\n",
    "# Initialize KFold cross-validator\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=43)\n",
    "\n",
    "# Initialize GridSearchCV with the DecisionTreeClassifier\n",
    "grid = GridSearchCV(DecisionTreeClassifier(class_weight=class_weights), param_grid=param_grid, cv=kf)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# Output the best parameters and best score\n",
    "print(\"The best parameters are %s with a score of %0.2f\" % (grid.best_params_, grid.best_score_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution in y_train: Counter({1: 2, 0: 1})\n",
      "Class 0 has only 1 member(s) in y_train.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# Check class distribution\n",
    "class_counts = Counter(y_train)\n",
    "print(f\"Class distribution in y_train: {class_counts}\")\n",
    "\n",
    "# Print classes with fewer than 2 members\n",
    "for cls, count in class_counts.items():\n",
    "    if count < 2:\n",
    "        print(f\"Class {cls} has only {count} member(s) in y_train.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique classes in y_train: [0 1]\n",
      "Accuracy of Decision Tree Classifier is: 0.00\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Check unique classes in y_train\n",
    "unique_classes = np.unique(y_train)\n",
    "print(f\"Unique classes in y_train: {unique_classes}\")\n",
    "\n",
    "# Only set class weights for classes that exist in y_train\n",
    "class_weights = {cls: 1 for cls in unique_classes}\n",
    "if 4 in unique_classes:\n",
    "    class_weights[4] = 2\n",
    "\n",
    "# Initialize the DecisionTreeClassifier with the appropriate class weights\n",
    "tree_model = DecisionTreeClassifier(class_weight=class_weights, max_depth=4, random_state=43)\n",
    "tree_model.fit(X_train, y_train)\n",
    "\n",
    "# Calculate and print the accuracy\n",
    "tree_accuracy = tree_model.score(X_test, y_test)\n",
    "print(f\"Accuracy of Decision Tree Classifier is: {tree_accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique classes in y_train: [0 1]\n",
      "Accuracy of Decision Tree Classifier is: 0.00\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Check unique classes in y_train\n",
    "unique_classes = np.unique(y_train)\n",
    "print(f\"Unique classes in y_train: {unique_classes}\")\n",
    "\n",
    "# Only set class weights for classes that exist in y_train\n",
    "class_weights = {cls: 1 for cls in unique_classes}\n",
    "if 4 in unique_classes:\n",
    "    class_weights[4] = 2\n",
    "\n",
    "# Initialize the DecisionTreeClassifier with the appropriate class weights\n",
    "tree_model = DecisionTreeClassifier(class_weight=class_weights, max_depth=4, random_state=43)\n",
    "tree_model.fit(X_train, y_train)\n",
    "\n",
    "# Calculate and print the accuracy\n",
    "tree_accuracy = tree_model.score(X_test, y_test)\n",
    "print(f\"Accuracy of Decision Tree Classifier is: {tree_accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZODAHYsfUDGB"
   },
   "source": [
    "We managed an improvement of over 4.58% in the overall accuracy score from 91.42% to 96.00%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jf8ua0wXUDGB"
   },
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique classes in y_train: [0 1]\n",
      "The best parameters are {'max_depth': 1, 'max_features': 1} with a score of 0.33\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "\n",
    "# Define the parameter grid for GridSearchCV\n",
    "max_depth_range = np.linspace(1, 10, 10, dtype=int)\n",
    "max_features_range = np.arange(1, 10, 1)\n",
    "param_grid = dict(max_depth=max_depth_range, max_features=max_features_range)\n",
    "\n",
    "# Get the number of samples in X_train\n",
    "num_samples = len(X_train)\n",
    "\n",
    "# Ensure the number of splits is less than or equal to the number of samples\n",
    "n_splits = min(5, num_samples)\n",
    "\n",
    "# Initialize KFold cross-validator\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=43)\n",
    "\n",
    "# Check unique classes in y_train\n",
    "unique_classes = np.unique(y_train)\n",
    "print(f\"Unique classes in y_train: {unique_classes}\")\n",
    "\n",
    "# Only set class weights for classes that exist in y_train\n",
    "class_weights = {cls: 1 for cls in unique_classes}\n",
    "if 4 in unique_classes:\n",
    "    class_weights[4] = 2\n",
    "\n",
    "# Initialize GridSearchCV with the RandomForestClassifier\n",
    "grid = GridSearchCV(RandomForestClassifier(class_weight=class_weights, random_state=43), param_grid=param_grid, cv=kf)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# Output the best parameters and best score\n",
    "print(\"The best parameters are %s with a score of %0.2f\" % (grid.best_params_, grid.best_score_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique classes in y_train: [0 1]\n",
      "Cross-validation scores: [0. 1. 0.]\n",
      "Mean cross-validation score: 0.3333333333333333\n",
      "Accuracy of Random Forest Classifier on test data: 0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Ensure X_train, y_train, X_test, y_test are properly loaded or generated here\n",
    "\n",
    "# Check unique classes in y_train\n",
    "unique_classes = np.unique(y_train)\n",
    "print(f\"Unique classes in y_train: {unique_classes}\")\n",
    "\n",
    "# Only set class weights for classes that exist in y_train\n",
    "class_weights = {cls: 1 for cls in unique_classes}\n",
    "if 4 in unique_classes:\n",
    "    class_weights[4] = 2\n",
    "\n",
    "# Initialize and train the Random Forest Classifier\n",
    "rf_model = RandomForestClassifier(class_weight=class_weights, max_depth=4, n_estimators=300, max_features=2, random_state=43, n_jobs=-1)\n",
    "\n",
    "# Check the size of the training set\n",
    "n_samples = X_train.shape[0]\n",
    "\n",
    "if n_samples < 5:\n",
    "    # If there are fewer than 5 samples, use leave-one-out cross-validation\n",
    "    from sklearn.model_selection import LeaveOneOut\n",
    "    loo = LeaveOneOut()\n",
    "    cv_scores = cross_val_score(rf_model, X_train, y_train, cv=loo)\n",
    "else:\n",
    "    # Use 5-fold cross-validation if there are enough samples\n",
    "    cv_scores = cross_val_score(rf_model, X_train, y_train, cv=5)\n",
    "\n",
    "# Fit the model on the training data\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate on the test data\n",
    "y_pred = rf_model.predict(X_test)\n",
    "rf_accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Cross-validation scores: {cv_scores}\")\n",
    "print(f\"Mean cross-validation score: {np.mean(cv_scores)}\")\n",
    "print(f\"Accuracy of Random Forest Classifier on test data: {rf_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FEKgjIy_UDGB"
   },
   "source": [
    "We are unable to improve score from 97.71%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F1GJjAXCUDGB"
   },
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "bdsjsmhrUDGB",
    "outputId": "96ad0c60-6122-4e55-be1a-5036d2f9cf4d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of MLP Classifier is:1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "nn_model = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(100,), random_state=43, max_iter=1000, learning_rate='adaptive')\n",
    "nn_model.fit(X_train_scaled, y_train)\n",
    "nn_accuracy = nn_model.score(X_test_scaled, y_test)\n",
    "print(f\"Accuracy of MLP Classifier is:{nn_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WikEEMAhUDGB"
   },
   "source": [
    "We managed an improvement of over 5.85% in the overall accuracy score from 88.57% to 94.42%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YPFgnwIDUDGB"
   },
   "source": [
    "### AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "SOABBSHvUDGC",
    "outputId": "1fa85c45-e36f-44c3-e0e1-37ada73fd5d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Ada Boost Classifier is:1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "ada_model = AdaBoostClassifier(random_state=43, n_estimators=100)\n",
    "ada_model.fit(X_train, y_train)\n",
    "ada_accuracy = ada_model.score(X_test, y_test)\n",
    "print(f\"Accuracy of Ada Boost Classifier is:{ada_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ziefWkCbUDGC"
   },
   "source": [
    "We managed an improvement of over 0.58% in the overall accuracy score from 95.42% to 96.00%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "koZHCKmxUDGC"
   },
   "source": [
    "### Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "ZIolo4w1UDGC",
    "outputId": "2341cede-82e1-44eb-ad62-46720ca2649c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Gaussian Naive Bayes Classifier is:0.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "gnb_model = GaussianNB()\n",
    "gnb_model.fit(X_train, y_train)\n",
    "gnb_accuracy = gnb_model.score(X_test, y_test)\n",
    "print(f\"Accuracy of Gaussian Naive Bayes Classifier is:{gnb_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QePz38C_UDGC"
   },
   "source": [
    "We are unable to improve score from 96.00%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AAaAFg7tUDGC"
   },
   "source": [
    "### Quadratic Discriminant Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique classes in y_train: [0 1]\n",
      "Class counts in y_train: [1 2]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Class 0 has only 1 sample(s), which is insufficient for QDA.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[82], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mcls\u001b[39m, count \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(unique_classes, class_counts):\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m count \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m---> 16\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClass \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has only \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcount\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m sample(s), which is insufficient for QDA.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Initialize and train the Quadratic Discriminant Analysis model\u001b[39;00m\n\u001b[0;32m     19\u001b[0m qda_model \u001b[38;5;241m=\u001b[39m QuadraticDiscriminantAnalysis()\n",
      "\u001b[1;31mValueError\u001b[0m: Class 0 has only 1 sample(s), which is insufficient for QDA."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Ensure X_train, y_train, X_test, y_test are properly loaded or generated here\n",
    "\n",
    "# Check unique classes and their counts in y_train\n",
    "unique_classes, class_counts = np.unique(y_train, return_counts=True)\n",
    "print(f\"Unique classes in y_train: {unique_classes}\")\n",
    "print(f\"Class counts in y_train: {class_counts}\")\n",
    "\n",
    "# Ensure each class has at least 2 samples\n",
    "for cls, count in zip(unique_classes, class_counts):\n",
    "    if count < 2:\n",
    "        raise ValueError(f\"Class {cls} has only {count} sample(s), which is insufficient for QDA.\")\n",
    "\n",
    "# Initialize and train the Quadratic Discriminant Analysis model\n",
    "qda_model = QuadraticDiscriminantAnalysis()\n",
    "qda_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate on the test data\n",
    "y_pred = qda_model.predict(X_test)\n",
    "qda_accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy of Quadratic Discriminant Analysis Classifier is: {qda_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique classes in y_train: [0 1]\n",
      "Class counts in y_train: [1 2]\n",
      "Unique classes in y_train_resampled: [0 1]\n",
      "Class counts in y_train_resampled: [2 2]\n",
      "Accuracy of Quadratic Discriminant Analysis Classifier is: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dhaka\\anaconda3\\Lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\dhaka\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:457: UserWarning: X has feature names, but QuadraticDiscriminantAnalysis was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\dhaka\\anaconda3\\Lib\\site-packages\\sklearn\\discriminant_analysis.py:960: RuntimeWarning: divide by zero encountered in power\n",
      "  X2 = np.dot(Xm, R * (S ** (-0.5)))\n",
      "C:\\Users\\dhaka\\anaconda3\\Lib\\site-packages\\sklearn\\discriminant_analysis.py:960: RuntimeWarning: invalid value encountered in multiply\n",
      "  X2 = np.dot(Xm, R * (S ** (-0.5)))\n",
      "C:\\Users\\dhaka\\anaconda3\\Lib\\site-packages\\sklearn\\discriminant_analysis.py:963: RuntimeWarning: divide by zero encountered in log\n",
      "  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Ensure X_train, y_train, X_test, y_test are properly loaded or generated here\n",
    "\n",
    "# Check unique classes and their counts in y_train\n",
    "unique_classes, class_counts = np.unique(y_train, return_counts=True)\n",
    "print(f\"Unique classes in y_train: {unique_classes}\")\n",
    "print(f\"Class counts in y_train: {class_counts}\")\n",
    "\n",
    "# Ensure each class has at least 2 samples by oversampling\n",
    "X_train_resampled = X_train.copy()\n",
    "y_train_resampled = y_train.copy()\n",
    "\n",
    "for cls in unique_classes:\n",
    "    cls_count = np.sum(y_train == cls)\n",
    "    if cls_count < 2:\n",
    "        # Get samples of the current class\n",
    "        cls_samples = X_train[y_train == cls]\n",
    "        cls_labels = y_train[y_train == cls]\n",
    "        # Resample to add one more sample\n",
    "        X_resampled, y_resampled = resample(cls_samples, cls_labels, replace=True, n_samples=2-cls_count, random_state=42)\n",
    "        # Append resampled data to the training set\n",
    "        X_train_resampled = np.vstack((X_train_resampled, X_resampled))\n",
    "        y_train_resampled = np.hstack((y_train_resampled, y_resampled))\n",
    "\n",
    "# Check new unique classes and their counts in y_train_resampled\n",
    "unique_classes_resampled, class_counts_resampled = np.unique(y_train_resampled, return_counts=True)\n",
    "print(f\"Unique classes in y_train_resampled: {unique_classes_resampled}\")\n",
    "print(f\"Class counts in y_train_resampled: {class_counts_resampled}\")\n",
    "\n",
    "# Initialize and train the Quadratic Discriminant Analysis model\n",
    "qda_model = QuadraticDiscriminantAnalysis()\n",
    "qda_model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Predict and evaluate on the test data\n",
    "y_pred = qda_model.predict(X_test)\n",
    "qda_accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy of Quadratic Discriminant Analysis Classifier is: {qda_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique classes in y_train: [0 1]\n",
      "Class counts in y_train: [1 2]\n",
      "Accuracy of Quadratic Discriminant Analysis Classifier is: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dhaka\\anaconda3\\Lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\dhaka\\anaconda3\\Lib\\site-packages\\sklearn\\discriminant_analysis.py:960: RuntimeWarning: divide by zero encountered in power\n",
      "  X2 = np.dot(Xm, R * (S ** (-0.5)))\n",
      "C:\\Users\\dhaka\\anaconda3\\Lib\\site-packages\\sklearn\\discriminant_analysis.py:960: RuntimeWarning: invalid value encountered in multiply\n",
      "  X2 = np.dot(Xm, R * (S ** (-0.5)))\n",
      "C:\\Users\\dhaka\\anaconda3\\Lib\\site-packages\\sklearn\\discriminant_analysis.py:963: RuntimeWarning: divide by zero encountered in log\n",
      "  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import resample\n",
    "from sklearn.preprocessing import StandardScaler  # Import StandardScaler for preprocessing\n",
    "\n",
    "# Your data loading or generation code here\n",
    "\n",
    "# Apply preprocessing steps (e.g., scaling) if needed\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Check unique classes and their counts in y_train\n",
    "unique_classes, class_counts = np.unique(y_train, return_counts=True)\n",
    "print(f\"Unique classes in y_train: {unique_classes}\")\n",
    "print(f\"Class counts in y_train: {class_counts}\")\n",
    "\n",
    "# Ensure each class has at least 2 samples by oversampling\n",
    "X_train_resampled = X_train_scaled.copy()  # Use scaled features for resampling\n",
    "y_train_resampled = y_train.copy()\n",
    "\n",
    "for cls in unique_classes:\n",
    "    cls_count = np.sum(y_train == cls)\n",
    "    if cls_count < 2:\n",
    "        # Get samples of the current class\n",
    "        cls_samples = X_train_scaled[y_train == cls]\n",
    "        cls_labels = y_train[y_train == cls]\n",
    "        # Resample to add one more sample\n",
    "        X_resampled, y_resampled = resample(cls_samples, cls_labels, replace=True, n_samples=2-cls_count, random_state=42)\n",
    "        # Append resampled data to the training set\n",
    "        X_train_resampled = np.vstack((X_train_resampled, X_resampled))\n",
    "        y_train_resampled = np.hstack((y_train_resampled, y_resampled))\n",
    "\n",
    "# Initialize and train the Quadratic Discriminant Analysis model\n",
    "qda_model = QuadraticDiscriminantAnalysis()\n",
    "qda_model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Predict and evaluate on the test data\n",
    "y_pred = qda_model.predict(X_test_scaled)  # Use scaled features for prediction\n",
    "qda_accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy of Quadratic Discriminant Analysis Classifier is: {qda_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique classes in y_train: [0 1]\n",
      "Class counts in y_train: [4 2]\n",
      "Accuracy of Quadratic Discriminant Analysis Classifier is: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dhaka\\anaconda3\\Lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import resample\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Your data loading or generation code here\n",
    "# Example (replace with your actual data loading):\n",
    "# X, y = load_your_data()\n",
    "\n",
    "# Create a sample dataset for demonstration\n",
    "data = {'feature1': [1, 2, 3, 4, 5, 6, 7, 8],\n",
    "        'feature2': ['A', 'B', 'C', 'A', 'B', 'C', 'A', 'B'],\n",
    "        'label': [0, 1, 0, 1, 0, 1, 0, 1]}\n",
    "df = pd.DataFrame(data)\n",
    "X = df[['feature1', 'feature2']]\n",
    "y = df['label']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the preprocessing steps\n",
    "numeric_features = ['feature1']\n",
    "categorical_features = ['feature2']\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Apply preprocessing steps (e.g., scaling, encoding)\n",
    "X_train_preprocessed = preprocessor.fit_transform(X_train)\n",
    "X_test_preprocessed = preprocessor.transform(X_test)\n",
    "\n",
    "# Check unique classes and their counts in y_train\n",
    "unique_classes, class_counts = np.unique(y_train, return_counts=True)\n",
    "print(f\"Unique classes in y_train: {unique_classes}\")\n",
    "print(f\"Class counts in y_train: {class_counts}\")\n",
    "\n",
    "# Ensure each class has at least 2 samples by oversampling\n",
    "X_train_resampled = X_train_preprocessed.copy()  # Use preprocessed features for resampling\n",
    "y_train_resampled = y_train.copy()\n",
    "\n",
    "for cls in unique_classes:\n",
    "    cls_count = np.sum(y_train == cls)\n",
    "    if cls_count < 2:\n",
    "        # Get samples of the current class\n",
    "        cls_samples = X_train_preprocessed[y_train == cls]\n",
    "        cls_labels = y_train[y_train == cls]\n",
    "        # Resample to add one more sample\n",
    "        X_resampled, y_resampled = resample(cls_samples, cls_labels, replace=True, n_samples=2-cls_count, random_state=42)\n",
    "        # Append resampled data to the training set\n",
    "        X_train_resampled = np.vstack((X_train_resampled, X_resampled))\n",
    "        y_train_resampled = np.hstack((y_train_resampled, y_resampled))\n",
    "\n",
    "# Apply PCA to reduce multicollinearity (optional, adjust n_components as needed)\n",
    "pca = PCA(n_components=0.95)  # Retain 95% of variance\n",
    "X_train_resampled_pca = pca.fit_transform(X_train_resampled)\n",
    "X_test_preprocessed_pca = pca.transform(X_test_preprocessed)\n",
    "\n",
    "# Initialize and train the Quadratic Discriminant Analysis model with shrinkage\n",
    "qda_model = QuadraticDiscriminantAnalysis(store_covariance=True, reg_param=0.1)\n",
    "qda_model.fit(X_train_resampled_pca, y_train_resampled)\n",
    "\n",
    "# Predict and evaluate on the test data\n",
    "y_pred = qda_model.predict(X_test_preprocessed_pca)  # Use PCA-transformed features for prediction\n",
    "qda_accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy of Quadratic Discriminant Analysis Classifier is: {qda_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique classes in y_train: [0 1]\n",
      "Class counts in y_train: [3 3]\n",
      "Accuracy of Quadratic Discriminant Analysis Classifier is: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dhaka\\anaconda3\\Lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import resample\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Create a sample dataset for demonstration\n",
    "data = {'feature1': [1, 2, 3, 4, 5, 6, 7, 8],\n",
    "        'feature2': ['A', 'B', 'C', 'A', 'B', 'C', 'A', 'B'],\n",
    "        'label': [0, 1, 0, 1, 0, 1, 0, 1]}\n",
    "df = pd.DataFrame(data)\n",
    "X = df[['feature1', 'feature2']]\n",
    "y = df['label']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Define the preprocessing steps\n",
    "numeric_features = ['feature1']\n",
    "categorical_features = ['feature2']\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Apply preprocessing steps (e.g., scaling, encoding)\n",
    "X_train_preprocessed = preprocessor.fit_transform(X_train)\n",
    "X_test_preprocessed = preprocessor.transform(X_test)\n",
    "\n",
    "# Check unique classes and their counts in y_train\n",
    "unique_classes, class_counts = np.unique(y_train, return_counts=True)\n",
    "print(f\"Unique classes in y_train: {unique_classes}\")\n",
    "print(f\"Class counts in y_train: {class_counts}\")\n",
    "\n",
    "# Ensure each class has at least 2 samples by oversampling\n",
    "X_train_resampled = X_train_preprocessed.copy()  # Use preprocessed features for resampling\n",
    "y_train_resampled = y_train.copy()\n",
    "\n",
    "for cls in unique_classes:\n",
    "    cls_count = np.sum(y_train == cls)\n",
    "    if cls_count < 2:\n",
    "        # Get samples of the current class\n",
    "        cls_samples = X_train_preprocessed[y_train == cls]\n",
    "        cls_labels = y_train[y_train == cls]\n",
    "        # Resample to add one more sample\n",
    "        X_resampled, y_resampled = resample(cls_samples, cls_labels, replace=True, n_samples=2-cls_count, random_state=42)\n",
    "        # Append resampled data to the training set\n",
    "        X_train_resampled = np.vstack((X_train_resampled, X_resampled))\n",
    "        y_train_resampled = np.hstack((y_train_resampled, y_resampled))\n",
    "\n",
    "# Apply PCA to reduce multicollinearity (optional, adjust n_components as needed)\n",
    "pca = PCA(n_components=0.95)  # Retain 95% of variance\n",
    "X_train_resampled_pca = pca.fit_transform(X_train_resampled)\n",
    "X_test_preprocessed_pca = pca.transform(X_test_preprocessed)\n",
    "\n",
    "# Initialize and train the Quadratic Discriminant Analysis model with shrinkage\n",
    "qda_model = QuadraticDiscriminantAnalysis(store_covariance=True, reg_param=0.1)\n",
    "qda_model.fit(X_train_resampled_pca, y_train_resampled)\n",
    "\n",
    "# Predict and evaluate on the test data\n",
    "y_pred = qda_model.predict(X_test_preprocessed_pca)  # Use PCA-transformed features for prediction\n",
    "qda_accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy of Quadratic Discriminant Analysis Classifier is: {qda_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import resample\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QDJ4xQGTUDGC"
   },
   "source": [
    "We are unable to improve score from 95.42%."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
